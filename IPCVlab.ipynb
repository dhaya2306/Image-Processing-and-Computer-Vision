{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "042812f6-b025-43a9-9478-b88a05ee1295",
   "metadata": {},
   "source": [
    "EXP 6-Recognize traffic sign using ML algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de69ef35-63c1-495a-b8da-31d87ee9887e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc2622b3-77bd-4860-b5b8-9623ebaa8c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized Traffic Sign: Speed Limit Sign\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def load_and_prepare_image(image_path):\n",
    "    # Load the image in grayscale\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    # Resize the image to a fixed size\n",
    "    image = cv2.resize(image, (100, 100))\n",
    "    # Flatten the image to a 1D array of pixel values\n",
    "    return image.flatten()\n",
    "\n",
    "def train_classifier(sign_paths):\n",
    "    # Prepare data for training\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for label, path in sign_paths.items():\n",
    "        # Load each sign image and flatten\n",
    "        features = load_and_prepare_image(path)\n",
    "        data.append(features)\n",
    "        labels.append(label)\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    labels_encoded = le.fit_transform(labels)\n",
    "    \n",
    "    # Initialize and train the Random Forest classifier\n",
    "    classifier = RandomForestClassifier()\n",
    "    classifier.fit(data, labels_encoded)\n",
    "    \n",
    "    return classifier, le\n",
    "\n",
    "def recognize_traffic_sign(input_image_path, classifier, label_encoder):\n",
    "    # Prepare the input image\n",
    "    input_features = load_and_prepare_image(input_image_path).reshape(1, -1)\n",
    "    \n",
    "    # Predict the label for the input image\n",
    "    label_encoded = classifier.predict(input_features)\n",
    "    \n",
    "    # Decode the label back to the original label name\n",
    "    recognized_sign = label_encoder.inverse_transform(label_encoded)[0]\n",
    "    \n",
    "    return recognized_sign\n",
    "\n",
    "# Paths to the sign images\n",
    "sign_paths = {\n",
    "    'Stop Sign': \"C:/Users/Admin/Downloads/ipcv/1/stop.png\",\n",
    "    'right Sign': \"C:/Users/Admin/Downloads/ipcv/3/right.jpeg\",\n",
    "    'Speed Limit Sign': \"C:/Users/Admin/Downloads/ipcv/2/speed.jpeg\",\n",
    "    'No Entry Sign': \"C:/Users/Admin/Downloads/ipcv/4/entry.jpeg\"\n",
    "}\n",
    "input_image_path = \"C:/Users/Admin/Downloads/ipcv/2/speed.jpeg\"\n",
    "\n",
    "# Train the classifier\n",
    "classifier, label_encoder = train_classifier(sign_paths)\n",
    "\n",
    "# Run the recognition\n",
    "recognized_sign = recognize_traffic_sign(input_image_path, classifier, label_encoder)\n",
    "\n",
    "if recognized_sign:\n",
    "    print(f\"Recognized Traffic Sign: {recognized_sign}\")\n",
    "else:\n",
    "    print(\"No matching traffic sign found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d027862-efc6-4965-a959-563a0de28a4a",
   "metadata": {},
   "source": [
    "EXP 9: Classify digits in MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2cfac140-ef85-483f-ac6e-63abfd360ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1750/1750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 14ms/step - accuracy: 0.9027 - loss: 0.3061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9830 - loss: 0.0542\n",
      "Test Accuracy: 98.26%\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000018F069CA8E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000018F069CA8E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "Predicted Digit: 7\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load MNIST dataset and split into train/test\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape and normalize the dataset\n",
    "X_train = X_train.values.reshape(-1, 28, 28, 1) / 255.0\n",
    "X_test = X_test.values.reshape(-1, 28, 28, 1) / 255.0\n",
    "\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train_categorical, epochs=1,  batch_size=32)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('mnist_cnn_model.h5')\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test_categorical)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Function to recognize digit from image\n",
    "def recognize_digit(image_path):\n",
    "    image = cv2.resize(cv2.imread(image_path, cv2.IMREAD_GRAYSCALE), (28, 28))\n",
    "    image = image.reshape(1, 28, 28, 1) / 255.0\n",
    "    prediction = model.predict(image)\n",
    "    return np.argmax(prediction)\n",
    "\n",
    "# Example usage\n",
    "input_image_path = r\"C:\\Users\\Admin\\Downloads\\ipcv\\seven.jpeg\"\n",
    "print(f\"Predicted Digit: {recognize_digit(input_image_path)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602ec4b7-7365-4425-b2f3-167f042d8ec1",
   "metadata": {},
   "source": [
    "EXP 5: Detect foreground and background in video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57020bb5-a21b-4f84-83a5-6fa04dc70276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Initialize video capture and background subtractor\n",
    "video = cv2.VideoCapture(r\"C:\\Users\\Admin\\Downloads\\3399820155-preview.mp4\")\n",
    "bg_subtractor = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()  # Read a frame\n",
    "    if not ret: break  # Exit if there are no frames left\n",
    "\n",
    "    # Apply background subtraction\n",
    "    fg_mask = bg_subtractor.apply(frame)  \n",
    "    cv2.imshow('Foreground', fg_mask)  # Show foreground mask\n",
    "\n",
    "    # Exit if the ESC key is pressed\n",
    "    if cv2.waitKey(30) == 27: break  \n",
    "\n",
    "# Clean up\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b22af49-ef2b-4127-8ba1-724e188274c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread(r\"C:\\Users\\Admin\\Downloads\\ipcv\\circle.png\")\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "edges = cv2.Canny(gray, 50, 150)\n",
    "\n",
    "# Detect lines\n",
    "lines = cv2.HoughLinesP(edges, 1, np.pi / 180, 100)\n",
    "if lines is not None:\n",
    "    for line in lines:\n",
    "        x1, y1, x2, y2 = line[0]\n",
    "        cv2.line(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "# Detect circles\n",
    "circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1, 20, param1=50, param2=30)\n",
    "if circles is not None:\n",
    "    for x, y, r in np.uint16(np.around(circles[0])):\n",
    "        cv2.circle(image, (x, y), r, (255, 0, 0), 2)\n",
    "\n",
    "# Detect ellipses\n",
    "contours, _ = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "for contour in contours:\n",
    "    if len(contour) >= 5:\n",
    "        cv2.ellipse(image, cv2.fitEllipse(contour), (0, 0, 255), 2)\n",
    "\n",
    "cv2.imshow(\"Detected Lines, Circles, and Ellipses\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec2eefbe-9d92-4458-8100-5687ca92a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the image\n",
    "image_path = r\"C:\\Users\\Admin\\Downloads\\ipcv\\counter detection.jpeg\"  # Replace with your image path\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Convert to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Perform edge detection using Canny\n",
    "edges = cv2.Canny(blurred, 50, 150)\n",
    "# Find contours\n",
    "contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "contoured_image = image.copy()\n",
    "cv2.drawContours(contoured_image, contours, -1, (0, 255, 0), 2)  # Green color with thickness of 2\n",
    "# Show the original and contoured images\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Contours', contoured_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d2d6e9b-1cb2-4aea-b9bf-4d9dea324941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f22731b6-b235-4214-9c61-e940b2d6882f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting smiles\n",
      "  Downloading smiles-0.0.0-py3-none-any.whl.metadata (449 bytes)\n",
      "Requirement already satisfied: Flask<4.0.0,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from smiles) (3.0.3)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Flask<4.0.0,>=3.0.2->smiles) (3.0.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Flask<4.0.0,>=3.0.2->smiles) (3.1.4)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Flask<4.0.0,>=3.0.2->smiles) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Flask<4.0.0,>=3.0.2->smiles) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Flask<4.0.0,>=3.0.2->smiles) (1.6.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from click>=8.1.3->Flask<4.0.0,>=3.0.2->smiles) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Jinja2>=3.1.2->Flask<4.0.0,>=3.0.2->smiles) (2.1.3)\n",
      "Downloading smiles-0.0.0-py3-none-any.whl (1.7 kB)\n",
      "Installing collected packages: smiles\n",
      "Successfully installed smiles-0.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fe525d-2885-4db6-931f-784109a6d48e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
